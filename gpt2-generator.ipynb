{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37ebd3c-d88c-4445-8575-bbb622f9cd0c",
   "metadata": {},
   "source": [
    "# Task 1: Text Generation using with GPT2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c35f7-28fc-4f73-b0bf-d7874a71864a",
   "metadata": {},
   "source": [
    "### 1. Environment Setup: Installing Dependencies\n",
    "\n",
    "Before we begin, we need to install the essential Python libraries. This cell handles the installation of:\n",
    "\n",
    "* **PyTorch (`torch`, `torchvision`, `torchaudio`)**: An open-source machine learning framework that provides the fundamental building blocks for building and training neural networks. We install it with a specific index URL to ensure compatibility with the CUDA version available for GPU acceleration.\n",
    "* **Hugging Face Libraries**:\n",
    "    * `transformers`: Provides access to thousands of pre-trained models like GPT-2 and the tools needed to download, configure, and train them.\n",
    "    * `datasets`: A library for easily accessing and processing large datasets.\n",
    "    * `accelerate`: A library that simplifies running PyTorch training across different hardware setups (like single GPU, multiple GPUs, or TPUs) with minimal code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645a0b67-2493-46e7-848a-8c86d3a31dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (72.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torchvision) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: transformers in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (4.53.0)\n",
      "Requirement already satisfied: datasets in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (1.8.1)\n",
      "Requirement already satisfied: filelock in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: networkx in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (72.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afcfd33-30d9-4ea9-9e4a-e62dec166b36",
   "metadata": {},
   "source": [
    "### 2. Importing Core Libraries\n",
    "\n",
    "With the dependencies installed, we now import the specific modules and classes required for our task.\n",
    "\n",
    "* `torch`: The core PyTorch library.\n",
    "* From `transformers`:\n",
    "    * `GPT2Tokenizer`: Responsible for converting raw text into a format (tokens) that the GPT-2 model can understand and vice-versa.\n",
    "    * `GPT2LMHeadModel`: The GPT-2 model architecture with a language modeling head on top, which is essential for text generation.\n",
    "    * `TextDataset`: A utility class to handle loading text files for language modeling tasks.\n",
    "    * `DataCollatorForLanguageModeling`: A helper that takes tokenized samples from our dataset and groups them into batches for the model. It also handles padding.\n",
    "    * `Trainer` & `TrainingArguments`: High-level classes that manage the entire training and evaluation loop, abstracting away much of the boilerplate code.\n",
    "    * `pipeline`: A high-level utility for performing inference tasks easily.\n",
    "* `os`: A standard Python library for interacting with the operating system, which can be useful for managing files and directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4728734c-bba7-4b62-b65c-45ace89e4ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 11:04:31.952182: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-30 11:04:32.046993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751261672.084656    5828 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751261672.095008    5828 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751261672.175186    5828 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751261672.175195    5828 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751261672.175196    5828 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751261672.175197    5828 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-30 11:04:32.184981: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch #Importing the torch Library\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, pipeline\n",
    "'''\n",
    "Importing:\n",
    "GPT2Tokenizer\n",
    "GPT2LMHeadModel\n",
    "TextDataset\n",
    "DataCollatorForLanguageModeling\n",
    "Trainer\n",
    "TrainerArguments\n",
    "Pipeline\n",
    "'''\n",
    "import os #Importing the os module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233cd446-3cc5-4b94-ac49-1998955663c9",
   "metadata": {},
   "source": [
    "### 3. Verifying GPU Availability\n",
    "\n",
    "Training large language models like GPT-2 is computationally intensive and can be extremely slow on a CPU. A CUDA-enabled GPU can accelerate this process by orders of magnitude. This code block verifies that PyTorch can detect and utilize the available GPU.\n",
    "\n",
    "* `torch.cuda.is_available()`: Checks if a compatible NVIDIA GPU is found and if the installed PyTorch version has CUDA support.\n",
    "* If a GPU is not found, a `RuntimeError` is raised to halt execution.\n",
    "* If successful, it prints the name of the GPU, the current PyTorch version, and a detailed memory summary to confirm the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93f6874-5607-4cc0-9bfd-14c91c5472ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available True\n",
      "Using device: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "PyTorch version: 2.5.1+cu121\n",
      "GPU Memory Summary: |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For checking if GPU is available\n",
    "if not torch.cuda.is_available(): #If Torch discovers no GPU\n",
    "    raise RuntimeError(\"GPU Unavailable, Ensure PyTorch was installed with CUDA support.\")\n",
    "    #Raises a Runtime Error stating that the local device Has no CUDA enabled GPU\n",
    "\n",
    "print(\"CUDA Available\", torch.cuda.is_available()) #If CUDA is available prints \"CUDA Available\"\n",
    "print(\"Using device:\", torch.cuda.get_device_name()) #If CUDA is available prints the CUDA device being used\n",
    "print(\"PyTorch version:\", torch.__version__) #Prints the version of Torch being used\n",
    "print(\"GPU Memory Summary:\", torch.cuda.memory_summary()) #Prints the summary of the GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255cda7-a174-4d28-ba85-5d546f9c67c6",
   "metadata": {},
   "source": [
    "### 4. Loading the Pre-trained GPT-2 Model and Tokenizer\n",
    "\n",
    "We will use the standard `gpt2` model from the Hugging Face model hub as our starting point. This model has been pre-trained on a massive corpus of general text and already has a strong grasp of the English language.\n",
    "\n",
    "* `GPT2Tokenizer.from_pretrained(model_name)`: Downloads and loads the tokenizer that was specifically trained with the `gpt2` model.\n",
    "* `GPT2LMHeadModel.from_pretrained(model_name)`: Downloads and loads the pre-trained weights of the `gpt2` model.\n",
    "* **Handling the Padding Token**: GPT-2 does not have a default padding token. We set the `pad_token` to be the same as the `eos_token` (end-of-sequence token). This is a common practice to enable batching of sequences with different lengths.\n",
    "* `model.resize_token_embeddings()`: We resize the model's token embedding layer to match the tokenizer's vocabulary size, ensuring consistency after adding the padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f083c746-4147-4118-a78c-d50840d9f890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\"  #Defining the pretrained model's name\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)  #Defining the tokenizer for the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)  #Loading the pre-trained GPT-2 language model with a language modeling head\n",
    "\n",
    "#Set pad token (as GPT-2 does not have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token  #Assigns the padding token of the tokenizer to be the EOS (end-of-sequence) token\n",
    "model.resize_token_embeddings(len(tokenizer))  #Resizes the model's token embedding layer to match the new vocabulary size of the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023e157-11b2-4bc7-a671-a0deaf468b01",
   "metadata": {},
   "source": [
    "### 5. Preparing the Dataset for Fine-Tuning\n",
    "\n",
    "Now, we prepare our custom dataset (Shakespeare's text) for the model.\n",
    "\n",
    "* `load_dataset()`: This function wraps the `TextDataset` class from Hugging Face. It reads a text file (`shake.txt`), tokenizes its content, and splits it into smaller chunks or `block_size`. A `block_size` of 128 means the model will be trained on segments of 128 tokens at a time.\n",
    "* `DataCollatorForLanguageModeling`: This object is crucial for the training process. It intelligently creates batches of data from our dataset. By setting `mlm=False` (Masked Language Modeling), we specify that we are doing Causal Language Modeling (CLM), which is the standard for auto-regressive models like GPT-2. The collator will handle padding the batches so that all sequences in a batch have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98213f25-3d4c-4950-8cee-75270bd3ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    '''\n",
    "    Loads a text file into a format suitable for training a language model.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the training text file.\n",
    "    tokenizer (PreTrainedTokenizer): The tokenizer to use for encoding the text.\n",
    "    block_size (int): The maximum length of each input block after tokenization.\n",
    "\n",
    "    Returns:\n",
    "    TextDataset: A dataset object containing tokenized text in blocks.\n",
    "'''\n",
    "    return TextDataset( #Returns TextDataset\n",
    "        tokenizer=tokenizer, #Tokenizer used to tokenize the input text\n",
    "        file_path=file_path, #Path to the text file to be loaded\n",
    "        block_size=block_size, #Maximum sequence length per training example\n",
    "    )\n",
    "\n",
    "train_file = 'shake.txt' #Path to the text file used to train the language model\n",
    "\n",
    "#Load the dataset using the custom function and tokenizer\n",
    "dataset = load_dataset(train_file, tokenizer) #Defining the dataset and the tokenizer\n",
    "#Define a data collator that dynamically pads batches and prepares them for language modeling\n",
    "#'mlm=False' means this is for causal (auto-regressive) language modeling like GPT-2\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee964eb-1742-4fb3-906b-c83dc131af2b",
   "metadata": {},
   "source": [
    "### 6. Configuring Training Arguments\n",
    "\n",
    "The `TrainingArguments` class allows us to define all the hyperparameters and settings for the training process in a single object.\n",
    "\n",
    "* `output_dir`: The directory where the fine-tuned model checkpoints and final model will be saved.\n",
    "* `overwrite_output_dir`: If `True`, it will overwrite the content of the output directory.\n",
    "* `num_train_epochs`: The total number of times the model will iterate over the entire training dataset.\n",
    "* `per_device_train_batch_size`: The number of training examples to use in a single batch on one GPU.\n",
    "* `gradient_accumulation_steps`: The number of forward passes to perform before running a backward pass to update the model's weights. This effectively increases the batch size without using more memory.\n",
    "* `save_steps`: A model checkpoint will be saved every 500 training steps.\n",
    "* `save_total_limit`: This limits the total number of checkpoints saved. Older ones are deleted to save space.\n",
    "* `logging_steps`: How often to log training metrics like loss.\n",
    "* `fp16`: Enables mixed-precision training, which uses both 16-bit and 32-bit floating-point types to speed up training and reduce memory usage on compatible hardware (like modern NVIDIA GPUs).\n",
    "* `dataloader_pin_memory`: When set to `True`, it can speed up data transfer from the CPU to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e265c14f-b333-4e6b-82ee-58bdf5513797",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-shakespeare-finetuned2\",       #Directory to save the model and checkpoints\n",
    "    overwrite_output_dir=True,                        #Overwrites the output directory if it exists\n",
    "    num_train_epochs=5,                               #Number of training epochs (passes through the entire dataset)\n",
    "    per_device_train_batch_size=5,                    #Batch size per GPU/CPU during training\n",
    "    gradient_accumulation_steps=1,                    #Number of steps to accumulate gradients before updating model weights\n",
    "    save_steps=500,                                   #Save a checkpoint every 500 steps\n",
    "    save_total_limit=2,                               #Maximum number of checkpoints to keep (older ones are deleted)\n",
    "    logging_dir=\"./logs\",                             #Directory to store training logs for TensorBoard or other tools\n",
    "    logging_steps=100,                                #Log training metrics every 100 steps\n",
    "    fp16=True,                                        #Use 16-bit (mixed) precision training if supported by the hardware\n",
    "    report_to=\"none\",                                 #Disable integration with logging/reporting tools like WandB or TensorBoard\n",
    "    dataloader_pin_memory=True,                       #Improves performance by enabling faster data transfer to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0c94f-a471-4454-9744-03dd1d962dc4",
   "metadata": {},
   "source": [
    "### 7. Moving the Model to the GPU\n",
    "\n",
    "Before we can start training, we must ensure that the model is loaded onto the correct computational device.\n",
    "\n",
    "* `torch.device(...)`: This line creates a `device` object that points to the GPU (`cuda`) if one is available, or falls back to the CPU otherwise.\n",
    "* `model.to(device)`: This is a crucial step that moves all of the model's parameters and buffers to the selected device (in this case, the GPU), ensuring that all subsequent computations are performed on the accelerated hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4fc91b7-7865-4952-a745-15da2eaa30a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Selects GPU ('cuda') if available, otherwise defaults to CPU\n",
    "model.to(device)                                                       # Moves the model to the selected device (GPU or CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73409c9-d20c-4da6-ab47-b08f11518c1c",
   "metadata": {},
   "source": [
    "### 8. Initializing the Trainer\n",
    "\n",
    "The `Trainer` class is a powerful utility from the Hugging Face `transformers` library that handles the entire training loop. We initialize it by passing all the components we have prepared so far:\n",
    "\n",
    "* `model`: The GPT-2 model we loaded and moved to the GPU.\n",
    "* `args`: The `TrainingArguments` object containing all our training configurations.\n",
    "* `data_collator`: The data collator to create batches for training.\n",
    "* `train_dataset`: Our tokenized Shakespeare dataset.\n",
    "\n",
    "The `Trainer` will now manage everything from batching the data to calculating the loss, performing backpropagation, and updating the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1309fe9a-49a5-4fa0-992a-5f03a6512a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563d06b-dc84-4294-8023-10cbe11af99f",
   "metadata": {},
   "source": [
    "### 9. Commencing the Fine-Tuning Process\n",
    "\n",
    "This is the moment where the actual training happens. Calling `trainer.train()` starts the fine-tuning process.\n",
    "\n",
    "The `Trainer` will now:\n",
    "1.  Iterate through the dataset for the specified number of epochs.\n",
    "2.  Feed batches of data to the model.\n",
    "3.  Calculate the loss (a measure of how far off the model's predictions are from the actual text).\n",
    "4.  Adjust the model's internal weights to minimize this loss.\n",
    "\n",
    "The output will show the training progress, including the loss at each logging step. A decreasing loss indicates that the model is successfully learning the patterns in the Shakespearean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de3ae76c-862d-4730-b2df-0d1676ad6ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2640' max='2640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2640/2640 04:23, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.640800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.536200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.354100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.028100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2640, training_loss=3.261607976393266, metrics={'train_runtime': 263.6034, 'train_samples_per_second': 50.075, 'train_steps_per_second': 10.015, 'total_flos': 862263705600000.0, 'train_loss': 3.261607976393266, 'epoch': 5.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  #This starts the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397d1de-e344-4a20-9987-8653374b2118",
   "metadata": {},
   "source": [
    "### 10. Saving the Final Model and Tokenizer\n",
    "\n",
    "After the training is complete, it's essential to save our work. This saves the fine-tuned model weights and the tokenizer's configuration, allowing us to load it later for inference without needing to repeat the training process.\n",
    "\n",
    "* `trainer.save_model(...)`: This saves the learned weights of the model, along with its configuration file, to the specified directory.\n",
    "* `tokenizer.save_pretrained(...)`: This saves the tokenizer's vocabulary and configuration files to the same directory. This ensures that the exact same tokenization scheme is used when we later load the model for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42b97065-627d-4cf0-93f3-22f3b614e9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-shakespeare-finetuned2/tokenizer_config.json',\n",
       " './gpt2-shakespeare-finetuned2/special_tokens_map.json',\n",
       " './gpt2-shakespeare-finetuned2/vocab.json',\n",
       " './gpt2-shakespeare-finetuned2/merges.txt',\n",
       " './gpt2-shakespeare-finetuned2/added_tokens.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./gpt2-shakespeare-finetuned2\")         #Saves the fine-tuned model to the specified directory\n",
    "tokenizer.save_pretrained(\"./gpt2-shakespeare-finetuned2\")  #Saves the tokenizer configuration and vocabulary to the same directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a9cd2-c9ac-4511-abc3-97b104b8ab7d",
   "metadata": {},
   "source": [
    "### 11. Generating Text with the Fine-Tuned Model\n",
    "\n",
    "With our model fine-tuned and saved, we can now use it to generate text. The Hugging Face `pipeline` provides a simple and high-level API for this task.\n",
    "\n",
    "* `pipeline(\"text-generation\", ...)`: We create a text generation pipeline.\n",
    "* `model`: We load the model from the directory where we saved our fine-tuned version (`./gpt2-shakespeare-finetuned2`).\n",
    "* `tokenizer`: We load the corresponding tokenizer.\n",
    "* `device=0`: We assign the pipeline to run on the first GPU (`cuda:0`) for faster inference.\n",
    "* `generator()`: We call the pipeline with our desired `prompt`, `max_length` (the total length of the generated text), and `num_return_sequences` (how many different versions to generate).\n",
    "\n",
    "The output will be a piece of text that starts with our prompt and is completed by the model in a style that it learned from the works of Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a93465c-44d4-4e5e-99e0-5af8b1263ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare Quote: Originally Posted by I think the only way I would go about it is to say that I don't really believe in a god. I would like to believe that there is something beyond the divine, but it's really hard to explain why. I would like to believe that there is something beyond the divine, but it's really hard to explain why.\n",
      "\n",
      "You should have to explain why.\n",
      "\n",
      "I think there is something beyond the divine but it's really hard to explain why.\n",
      "\n",
      "\n",
      "I would like to believe that there is something beyond the divine but it's really hard to explain why.I think there is something beyond the divine but it's really hard to explain why.\n",
      "\n",
      "There is a real possibility that there is. Like the Lord is the Creator and will be forever. There is a real possibility that there is. Like the Lord is the Creator and will be forever.\n",
      "\n",
      "Sorcery (1/2):\n",
      "\n",
      "I think there is something beyond the divine but it's really hard to explain why. I would like to believe that there is something beyond the divine but it's really hard to explain why.I would like to believe that there is something beyond the divine but it's really hard to explain why.\n",
      "\n",
      "There is\n"
     ]
    }
   ],
   "source": [
    "#Generation\n",
    "generator = pipeline(\"text-generation\", model=\"./gpt2-shakespeare-finetuned\", tokenizer=tokenizer, device=0)  \n",
    "#Creates a text generation pipeline using the fine-tuned model and tokenizer\n",
    "#'device=0' assigns the pipeline to the first CUDA GPU (if available)\n",
    "\n",
    "prompt = \"Shakespeare Quote\"  #Input text prompt to start generation from (empty string means model generates from scratch)\n",
    "\n",
    "output = generator(prompt, max_length=100, num_return_sequences=1)  \n",
    "#Generates up to 100 tokens of text based on the prompt\n",
    "#Returns 1 generated sequence\n",
    "\n",
    "print(output[0][\"generated_text\"])  #Prints the generated text from the first sequence in the output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
