{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37ebd3c-d88c-4445-8575-bbb622f9cd0c",
   "metadata": {},
   "source": [
    "# Task 1: Text Generation using with GPT2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c35f7-28fc-4f73-b0bf-d7874a71864a",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645a0b67-2493-46e7-848a-8c86d3a31dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (72.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torchvision) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: transformers in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (4.53.0)\n",
      "Requirement already satisfied: datasets in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (1.8.1)\n",
      "Requirement already satisfied: filelock in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: networkx in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (72.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afcfd33-30d9-4ea9-9e4a-e62dec166b36",
   "metadata": {},
   "source": [
    "### Importing the needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4728734c-bba7-4b62-b65c-45ace89e4ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 11:04:31.952182: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-30 11:04:32.046993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751261672.084656    5828 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751261672.095008    5828 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751261672.175186    5828 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751261672.175195    5828 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751261672.175196    5828 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751261672.175197    5828 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-30 11:04:32.184981: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch #Importing the torch Library\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, pipeline\n",
    "'''\n",
    "Importing:\n",
    "GPT2Tokenizer\n",
    "GPT2LMHeadModel\n",
    "TextDataset\n",
    "DataCollatorForLanguageModeling\n",
    "Trainer\n",
    "TrainerArguments\n",
    "Pipeline\n",
    "'''\n",
    "import os #Importing the os module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233cd446-3cc5-4b94-ac49-1998955663c9",
   "metadata": {},
   "source": [
    "### Checking for GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93f6874-5607-4cc0-9bfd-14c91c5472ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available True\n",
      "Using device: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "PyTorch version: 2.5.1+cu121\n",
      "GPU Memory Summary: |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For checking if GPU is available\n",
    "if not torch.cuda.is_available(): #If Torch discovers no GPU\n",
    "    raise RuntimeError(\"GPU Unavailable, Ensure PyTorch was installed with CUDA support.\")\n",
    "    #Raises a Runtime Error stating that the local device Has no CUDA enabled GPU\n",
    "\n",
    "print(\"CUDA Available\", torch.cuda.is_available()) #If CUDA is available prints \"CUDA Available\"\n",
    "print(\"Using device:\", torch.cuda.get_device_name()) #If CUDA is available prints the CUDA device being used\n",
    "print(\"PyTorch version:\", torch.__version__) #Prints the version of Torch being used\n",
    "print(\"GPU Memory Summary:\", torch.cuda.memory_summary()) #Prints the summary of the GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255cda7-a174-4d28-ba85-5d546f9c67c6",
   "metadata": {},
   "source": [
    "###  Loading and Preparing Pretrained GPT-2 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f083c746-4147-4118-a78c-d50840d9f890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\"  # Defining the pretrained model's name\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)  # Defining the tokenizer for the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)  # Loading the pre-trained GPT-2 language model with a language modeling head\n",
    "\n",
    "# Set pad token (as GPT-2 does not have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Assigns the padding token of the tokenizer to be the EOS (end-of-sequence) token\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resizes the model's token embedding layer to match the new vocabulary size of the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023e157-11b2-4bc7-a671-a0deaf468b01",
   "metadata": {},
   "source": [
    "### Preparing the Dataset and Data Collator for Language Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98213f25-3d4c-4950-8cee-75270bd3ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heytanix/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    '''\n",
    "    Loads a text file into a format suitable for training a language model.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the training text file.\n",
    "    tokenizer (PreTrainedTokenizer): The tokenizer to use for encoding the text.\n",
    "    block_size (int): The maximum length of each input block after tokenization.\n",
    "\n",
    "    Returns:\n",
    "    TextDataset: A dataset object containing tokenized text in blocks.\n",
    "'''\n",
    "    return TextDataset( #Returns TextDataset\n",
    "        tokenizer=tokenizer, #Tokenizer used to tokenize the input text\n",
    "        file_path=file_path, #Path to the text file to be loaded\n",
    "        block_size=block_size, #Maximum sequence length per training example\n",
    "    )\n",
    "\n",
    "train_file = 'shake.txt' #Path to the text file used to train the language model\n",
    "\n",
    "# Load the dataset using the custom function and tokenizer\n",
    "dataset = load_dataset(train_file, tokenizer) #Defining the dataset and the tokenizer\n",
    "# Define a data collator that dynamically pads batches and prepares them for language modeling\n",
    "# 'mlm=False' means this is for causal (auto-regressive) language modeling like GPT-2\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee964eb-1742-4fb3-906b-c83dc131af2b",
   "metadata": {},
   "source": [
    "### Configuring Training Arguments for GPT-2 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e265c14f-b333-4e6b-82ee-58bdf5513797",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-shakespeare-finetuned2\",       # Directory to save the model and checkpoints\n",
    "    overwrite_output_dir=True,                        # Overwrites the output directory if it exists\n",
    "    num_train_epochs=5,                               # Number of training epochs (passes through the entire dataset)\n",
    "    per_device_train_batch_size=5,                    # Batch size per GPU/CPU during training\n",
    "    gradient_accumulation_steps=1,                    # Number of steps to accumulate gradients before updating model weights\n",
    "    save_steps=500,                                   # Save a checkpoint every 500 steps\n",
    "    save_total_limit=2,                               # Maximum number of checkpoints to keep (older ones are deleted)\n",
    "    logging_dir=\"./logs\",                             # Directory to store training logs for TensorBoard or other tools\n",
    "    logging_steps=100,                                # Log training metrics every 100 steps\n",
    "    fp16=True,                                        # Use 16-bit (mixed) precision training if supported by the hardware\n",
    "    report_to=\"none\",                                 # Disable integration with logging/reporting tools like WandB or TensorBoard\n",
    "    dataloader_pin_memory=True,                       # Improves performance by enabling faster data transfer to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0c94f-a471-4454-9744-03dd1d962dc4",
   "metadata": {},
   "source": [
    "### Setting Up Device for Model Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4fc91b7-7865-4952-a745-15da2eaa30a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Selects GPU ('cuda') if available, otherwise defaults to CPU\n",
    "model.to(device)                                                       # Moves the model to the selected device (GPU or CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73409c9-d20c-4da6-ab47-b08f11518c1c",
   "metadata": {},
   "source": [
    "### Saving the Fine-Tuned GPT-2 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1309fe9a-49a5-4fa0-992a-5f03a6512a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de3ae76c-862d-4730-b2df-0d1676ad6ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2640' max='2640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2640/2640 04:23, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.640800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.536200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.354100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.028100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2640, training_loss=3.261607976393266, metrics={'train_runtime': 263.6034, 'train_samples_per_second': 50.075, 'train_steps_per_second': 10.015, 'total_flos': 862263705600000.0, 'train_loss': 3.261607976393266, 'epoch': 5.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # This starts the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42b97065-627d-4cf0-93f3-22f3b614e9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-shakespeare-finetuned2/tokenizer_config.json',\n",
       " './gpt2-shakespeare-finetuned2/special_tokens_map.json',\n",
       " './gpt2-shakespeare-finetuned2/vocab.json',\n",
       " './gpt2-shakespeare-finetuned2/merges.txt',\n",
       " './gpt2-shakespeare-finetuned2/added_tokens.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./gpt2-shakespeare-finetuned2\")         # Saves the fine-tuned model to the specified directory\n",
    "tokenizer.save_pretrained(\"./gpt2-shakespeare-finetuned2\")  # Saves the tokenizer configuration and vocabulary to the same directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a9cd2-c9ac-4511-abc3-97b104b8ab7d",
   "metadata": {},
   "source": [
    "### Generating Text Using the Fine-Tuned GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a93465c-44d4-4e5e-99e0-5af8b1263ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare Quote: Originally Posted by I think the only way I would go about it is to say that I don't really believe in a god. I would like to believe that there is something beyond the divine, but it's really hard to explain why. I would like to believe that there is something beyond the divine, but it's really hard to explain why.\n",
      "\n",
      "You should have to explain why.\n",
      "\n",
      "I think there is something beyond the divine but it's really hard to explain why.\n",
      "\n",
      "\n",
      "I would like to believe that there is something beyond the divine but it's really hard to explain why.I think there is something beyond the divine but it's really hard to explain why.\n",
      "\n",
      "There is a real possibility that there is. Like the Lord is the Creator and will be forever. There is a real possibility that there is. Like the Lord is the Creator and will be forever.\n",
      "\n",
      "Sorcery (1/2):\n",
      "\n",
      "I think there is something beyond the divine but it's really hard to explain why. I would like to believe that there is something beyond the divine but it's really hard to explain why.I would like to believe that there is something beyond the divine but it's really hard to explain why.\n",
      "\n",
      "There is\n"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "generator = pipeline(\"text-generation\", model=\"./gpt2-shakespeare-finetuned\", tokenizer=tokenizer, device=0)  \n",
    "# Creates a text generation pipeline using the fine-tuned model and tokenizer\n",
    "# 'device=0' assigns the pipeline to the first CUDA GPU (if available)\n",
    "\n",
    "prompt = \"Shakespeare Quote\"  # Input text prompt to start generation from (empty string means model generates from scratch)\n",
    "\n",
    "output = generator(prompt, max_length=100, num_return_sequences=1)  \n",
    "# Generates up to 100 tokens of text based on the prompt\n",
    "# Returns 1 generated sequence\n",
    "\n",
    "print(output[0][\"generated_text\"])  # Prints the generated text from the first sequence in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472097ba-e491-4bf4-bccc-773f243e1ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
